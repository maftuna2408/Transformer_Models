{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmYGVziz50tu"
      },
      "source": [
        "- Remove non alphanumeric characters for simple training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1051,
      "metadata": {
        "id": "FOwRggVcwtzP"
      },
      "outputs": [],
      "source": [
        "from transformer import Transformer # this is the transformer.py file\n",
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1052,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) Fayl nomlarini belgilang\n",
        "english_file = r\"C:\\Users\\Maftuna\\Desktop\\English.txt\"\n",
        "turkish_file = r\"C:\\Users\\Maftuna\\Desktop\\Turkish.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1053,
      "metadata": {},
      "outputs": [],
      "source": [
        "PADDING_TOKEN = ''\n",
        "START_TOKEN = ''\n",
        "END_TOKEN = ''\n",
        "\n",
        "\n",
        "turkish_vocabulary = [\n",
        "    START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/',\n",
        "    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "    ':', ';', '<', '=', '>', '?', '@',\n",
        "    'A', 'B', 'C', 'Ç', 'D', 'E', 'F', 'G', 'Ğ', 'H', 'I', 'İ', 'J', 'K', 'L',\n",
        "    'M', 'N', 'O', 'Ö', 'P', 'Q', 'R', 'S', 'Ş', 'T', 'U', 'Ü', 'V', 'W', 'X', 'Y', 'Z',\n",
        "    '[', '\\\\', ']', '^', '_', '`',\n",
        "    'a', 'b', 'c', 'ç', 'd', 'e', 'f', 'g', 'ğ', 'h', 'ı', 'i', 'j', 'k', 'l',\n",
        "    'm', 'n', 'o', 'ö', 'p', 'q', 'r', 's', 'ş', 't', 'u', 'ü', 'v', 'w', 'x', 'y', 'z',\n",
        "    '{', '|', '}', '~',\n",
        "    '€', '£', '₺', '₽', '¥',  # pul birliklari\n",
        "    PADDING_TOKEN, END_TOKEN\n",
        "]\n",
        "\n",
        "index_to_turkish = {k:v for k,v in enumerate(turkish_vocabulary)}\n",
        "turkish_to_index = {v:k for k,v in enumerate(turkish_vocabulary)}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1054,
      "metadata": {},
      "outputs": [],
      "source": [
        "START_TOKEN = ''\n",
        "PADDING_TOKEN = ''\n",
        "END_TOKEN = ''\n",
        "\n",
        "english_vocabulary = [\n",
        "    START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/',\n",
        "    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "    ':', ';', '<', '=', '>', '?', '@',\n",
        "    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L',\n",
        "    'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
        "    '[', '\\\\', ']', '^', '_', '`',\n",
        "    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
        "    'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
        "    '{', '|', '}', '~',\n",
        "    PADDING_TOKEN, END_TOKEN\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1055,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Create mappings for Turkish vocabulary\n",
        "index_to_turkish = {k: v for k, v in enumerate(turkish_vocabulary)}\n",
        "turkish_to_index = {v: k for k, v in enumerate(turkish_vocabulary)}\n",
        "\n",
        "# Create mappings for English vocabulary\n",
        "index_to_english = {k: v for k, v in enumerate(english_vocabulary)}\n",
        "english_to_index = {v: k for k, v in enumerate(english_vocabulary)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1056,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(turkish_file, 'r', encoding='utf-8') as file:\n",
        "    turkish_sentences = file.readlines()\n",
        "with open(english_file, 'r', encoding='utf-8') as file:\n",
        "    english_sentences = file.readlines()\n",
        "\n",
        "# Limit Number of sentences\n",
        "TOTAL_SENTENCES = 5074\n",
        "english_sentences = english_sentences[:TOTAL_SENTENCES]\n",
        "turkish_sentences = turkish_sentences[:TOTAL_SENTENCES]\n",
        "english_sentences = [sentence.rstrip('\\n').lower() for sentence in english_sentences]\n",
        "turkish_sentences = [sentence.rstrip('\\n') for sentence in turkish_sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1057,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['the members of femen stripped their tops in a school in üsküdar (a conservative district of i̇stanbul) and shouted slogans on the table covered with voting cards.',\n",
              " 'it turned out the polling station subjected to femen protest was where pm recep tayyip erdoğan would cast his vote in i̇stanbul.',\n",
              " '\"femen activists stormed the voting papers where pm erdoğan would cast his vote.”',\n",
              " 'previously, femen turkey protested against the twitter ban in turkey with a femen activist sharing her bare-breast photo with \"#dns\" and \"#direntwitter” (resisttwitter) and dns numbers written on it. (ea/bd)',\n",
              " 'photo credit: \\u200f@vita__nova , twitter',\n",
              " \"as gunshots resounded in hakkari province bağlar neighborhood and other locations, one person, named mustafa er died and another was injured, according to news on yüksekovahaber\\xa0website. mustafa er's burial was held late last night (july 20) in bağlar neighborhood. 43-year-old er was reported to be a father of 9.\",\n",
              " 'ambulances carried four people injured at different stages of the events to van province.',\n",
              " 'tribal elders call to equanimity']"
            ]
          },
          "execution_count": 1057,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "english_sentences[:8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1058,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"FEMEN üyeleri Üsküdar'daki bir okulda soyunarak oy pusulalarının bulunduğu masaya çıkıp slogan attı. İki sekstivist sandık merkezinde görevli polisler tarafından gözaltına alındı.\",\n",
              " \"FEMEN aktivistleri Erdoğan'ın İstanbul'da oy kullanacağı yerde oy zarflarını fırlattı. #ErdoğanıYasaklayın pic.twitter.com/msuCqlQnAa\",\n",
              " '\"FEMEN aktivistleri Erdoğan\\'ın İstanbul\\'da oy kullanacağı yerde oy zarflarını fırlattı\"',\n",
              " 'FEMEN Türkiye, geçen hafta da Twitter’dan bir sekstivistin memeleri üzerinde \"#DNS\" ve \"#DirenTwitter\" hashtag\\'leri ve DNS sayıları yazılı fotoğraf paylaşarak Türkiye\\'deki twitter yasağını protesto etmişti. (EA)',\n",
              " 'Fotoğraf: \\u200f@Vita__Nova , Twitter',\n",
              " \"Yüksekovahaber sitesinin haberine göre Hakkari'nin Bağlar mahallesi ile farklı noktalarda silah sesleri yükselirken kavgada Mustafa Er isimli bir kişi hayatını kaybetti, bir kişi de yaralandı. Mustafa Er'in cenazesinin bu akşam Bağlar Mahallesi'nde toprağa verilmesi bekleniyor. 43 yaşındaki Er'in 9 çocuk babası olduğu öğrenildi.\",\n",
              " \"Olaylarda farklı zamanlarda yaralanan dört kişi ise ambulanslar ile Van'a sevk edildi.\",\n",
              " 'Aşiret büyüklerinden itidal çağrısı']"
            ]
          },
          "execution_count": 1058,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "turkish_sentences[:8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1059,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "97th percentile length Kannada: 403.0\n",
            "97th percentile length English: 367.619999999999\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "PERCENTILE = 97\n",
        "print( f\"{PERCENTILE}th percentile length Kannada: {np.percentile([len(x) for x in turkish_sentences], PERCENTILE)}\" )\n",
        "print( f\"{PERCENTILE}th percentile length English: {np.percentile([len(x) for x in english_sentences], PERCENTILE)}\" )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1060,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of sentences: 5074\n",
            "Number of valid sentences: 2263\n"
          ]
        }
      ],
      "source": [
        "max_sequence_length = 200\n",
        "\n",
        "def is_valid_tokens(sentence, vocab):\n",
        "    for token in list(set(sentence)):\n",
        "        if token not in vocab:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def is_valid_length(sentence, max_sequence_length):\n",
        "    return len(list(sentence)) < (max_sequence_length - 1) # need to re-add the end token so leaving 1 space\n",
        "\n",
        "valid_sentence_indicies = []\n",
        "for index in range(len(turkish_sentences)):\n",
        "    turkish_sentence, english_sentence = turkish_sentences[index], english_sentences[index]\n",
        "    if is_valid_length(turkish_sentence, max_sequence_length) \\\n",
        "      and is_valid_length(english_sentence, max_sequence_length) \\\n",
        "      and is_valid_tokens(turkish_sentence, turkish_vocabulary):\n",
        "        valid_sentence_indicies.append(index)\n",
        "\n",
        "print(f\"Number of sentences: {len(turkish_sentences)}\")\n",
        "print(f\"Number of valid sentences: {len(valid_sentence_indicies)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1061,
      "metadata": {},
      "outputs": [],
      "source": [
        "turkish_sentences = [turkish_sentences[i] for i in valid_sentence_indicies]\n",
        "english_sentences = [english_sentences[i] for i in valid_sentence_indicies]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1062,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"FEMEN üyeleri Üsküdar'daki bir okulda soyunarak oy pusulalarının bulunduğu masaya çıkıp slogan attı. İki sekstivist sandık merkezinde görevli polisler tarafından gözaltına alındı.\",\n",
              " \"FEMEN aktivistleri Erdoğan'ın İstanbul'da oy kullanacağı yerde oy zarflarını fırlattı. #ErdoğanıYasaklayın pic.twitter.com/msuCqlQnAa\",\n",
              " '\"FEMEN aktivistleri Erdoğan\\'ın İstanbul\\'da oy kullanacağı yerde oy zarflarını fırlattı\"',\n",
              " \"Olaylarda farklı zamanlarda yaralanan dört kişi ise ambulanslar ile Van'a sevk edildi.\",\n",
              " 'Aşiret büyüklerinden itidal çağrısı',\n",
              " 'Valilik: İkinci bir emre kadar sokağa çıkmak yasak']"
            ]
          },
          "execution_count": 1062,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "turkish_sentences[:6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1063,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "\n",
        "d_model = 512\n",
        "batch_size = 30\n",
        "ffn_hidden = 2048\n",
        "num_heads = 8\n",
        "drop_prob = 0.1\n",
        "num_layers = 1\n",
        "max_sequence_length = 200\n",
        "tr_vocab_size = len(turkish_vocabulary)\n",
        "\n",
        "transformer = Transformer(d_model, \n",
        "                          ffn_hidden,\n",
        "                          num_heads, \n",
        "                          drop_prob, \n",
        "                          num_layers, \n",
        "                          max_sequence_length,\n",
        "                          tr_vocab_size,\n",
        "                          english_to_index,\n",
        "                          turkish_to_index,\n",
        "                          START_TOKEN, \n",
        "                          END_TOKEN, \n",
        "                          PADDING_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1064,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): Encoder(\n",
              "    (sentence_embedding): SentenceEmbedding(\n",
              "      (embedding): Embedding(96, 512)\n",
              "      (position_encoder): PositionalEncoding()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (layers): SequentialEncoder(\n",
              "      (0): EncoderLayer(\n",
              "        (attention): MultiHeadAttention(\n",
              "          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm1): LayerNormalization()\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (ffn): PositionwiseFeedForward(\n",
              "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (norm2): LayerNormalization()\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (sentence_embedding): SentenceEmbedding(\n",
              "      (embedding): Embedding(113, 512)\n",
              "      (position_encoder): PositionalEncoding()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (layers): SequentialDecoder(\n",
              "      (0): DecoderLayer(\n",
              "        (self_attention): MultiHeadAttention(\n",
              "          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (layer_norm1): LayerNormalization()\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (encoder_decoder_attention): MultiHeadCrossAttention(\n",
              "          (kv_layer): Linear(in_features=512, out_features=1024, bias=True)\n",
              "          (q_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (layer_norm2): LayerNormalization()\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (ffn): PositionwiseFeedForward(\n",
              "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (layer_norm3): LayerNormalization()\n",
              "        (dropout3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (linear): Linear(in_features=512, out_features=115, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 1064,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1065,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "\n",
        "    def __init__(self, english_sentences, turkish_sentences):\n",
        "        self.english_sentences = english_sentences\n",
        "        self.turkish_sentences = turkish_sentences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.english_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.english_sentences[idx], self.turkish_sentences[idx]\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1066,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "dataset = TextDataset(english_sentences, turkish_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1067,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2263"
            ]
          },
          "execution_count": 1067,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1068,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('it turned out the polling station subjected to femen protest was where pm recep tayyip erdoğan would cast his vote in i̇stanbul.',\n",
              " \"FEMEN aktivistleri Erdoğan'ın İstanbul'da oy kullanacağı yerde oy zarflarını fırlattı. #ErdoğanıYasaklayın pic.twitter.com/msuCqlQnAa\")"
            ]
          },
          "execution_count": 1068,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "dataset[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1069,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "train_loader = DataLoader(dataset, batch_size)\n",
        "iterator = iter(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1070,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('the members of femen stripped their tops in a school in üsküdar (a conservative district of i̇stanbul) and shouted slogans on the table covered with voting cards.', 'it turned out the polling station subjected to femen protest was where pm recep tayyip erdoğan would cast his vote in i̇stanbul.', '\"femen activists stormed the voting papers where pm erdoğan would cast his vote.”', 'ambulances carried four people injured at different stages of the events to van province.', 'tribal elders call to equanimity', 'governorate: curfew until further notice', 'entering or exiting the town center has been forbidden. lots of vehicles with citizens wanting to reach the town center are being held at depin police checkpoint.', 'dbp provincial co-chair: provocation', 'democratic regions party (dbp) province co-chair m. miraz çallı made the declaration on the inter-tribal conflict:', 'the southeast (turabdin) assyrian culture and solidarity association opened the center as part of the project backed by the european union and human rights program.', 'the aim is to ensure the continuity of the center by founding an assyrian women’s association after the project is over.', '“the assyrian woman is twice as reserved”', 'project assistant ninve özgün specified that they want to lay bare the introverted assyrian women’s problems and incorporate the women into social life.', 'learning to write in one’s mother language', 'a people unable to speak with its neighbors', 'project coordinator tuma çelik called to mind that a large portion of assyrians living in the mesopotamian region for 7,000 years were made to immigrate.', '25,000 assyrians remaining', 'assyrians living in turkey were forced to leave their homeland of centuries due to the 1915 genocide and the conflict between the kurdish workers’ party (pkk) and turkish armed forces in the 1990’s.', 'hydroelectric plants not permitted', 'the process of the struggle', 'nevertheless, antalya culture and nature properties protection board and hydroelectric plant companies objected to the verdict.', '11:00 am\\xa0police officers in provinces such as\\xa0diyarbakır, i̇zmir, van, ankara, and bitlis have been detained as part of the operation.', '10:00 am today (july 22): the number of detentions reached 87.', 'istanbul fight against terror branch is running the istanbul-centered operation on 200 addresses in 22 provinces and has so far detained around 52 suspects.', '40 people were detained in the first operation that started last night at 1:50 am.', 'authorities announced detainees are charged with founding organizations to commit crime, illegal wiretapping, and forgery.', 'among those detained are former fight against terror branch head yurt atayün and the same period’s fight against terror branch head ömer köse.', 'then istanbul intelligence branch head ali fuat yılmazer and many intelligence officials’ houses are also undergoing search.', '* bianet compiled this news item from newspapers’ websites.', 'yiğit şahan’s treatment is ongoing in şanlıurfa harran university research and practice hospital.'), (\"FEMEN üyeleri Üsküdar'daki bir okulda soyunarak oy pusulalarının bulunduğu masaya çıkıp slogan attı. İki sekstivist sandık merkezinde görevli polisler tarafından gözaltına alındı.\", \"FEMEN aktivistleri Erdoğan'ın İstanbul'da oy kullanacağı yerde oy zarflarını fırlattı. #ErdoğanıYasaklayın pic.twitter.com/msuCqlQnAa\", '\"FEMEN aktivistleri Erdoğan\\'ın İstanbul\\'da oy kullanacağı yerde oy zarflarını fırlattı\"', \"Olaylarda farklı zamanlarda yaralanan dört kişi ise ambulanslar ile Van'a sevk edildi.\", 'Aşiret büyüklerinden itidal çağrısı', 'Valilik: İkinci bir emre kadar sokağa çıkmak yasak', 'İl merkezine giriş çıkışların yasaklandı. İl merkezine gitmek isteyen yurttaşların içerisinde bulunduğu çok sayıda araç Depin Polis Kontrol Noktasında bekletiliyor.', 'DBP il eşbaşkanı: Provokasyon', \"Demokratik Bölgeler Partisi'nden (DBP) aşiretler arası çatışmayla ilgili açıklamayı il eş başkanı M. Miraz Çallı yaptı:\", \"Merkez, Güneydoğu (Turabdin) Süryani Kültür ve Dayanışma Derneği tarafından, Avrupa Birliği ve İnsan Hakları Programı'nın desteklediği proje kapsamında açıldı.\", 'Proje bittikten sonra da merkezin sürekliliği sağlanarak Süryani Kadın Derneği kurulması amaçlanıyor.', 'Süryani kadını iki kat fazla içe kapanık', 'Proje Asistanı Ninve Özgün, içe kapanık Süryani kadınlarının sorunlarını ortaya koymak ve onları sosyal hayata katmayı istediklerini belirtti.', 'Anadillerinde yazmayı öğrendiler', 'Komşusuyla bile konuşamayan bir halk', 'Proje Koordinatörü Tuma Çelik, Mezopotamya bölgesinde 7 bin yıldır yaşayan Süryanilerin büyük bir bölümünün göç etmek zorunda bırakıldığını hatırlattı.', '25 bin Süryani kaldı', \"Türkiye'de yaşayan Süryaniler 1915 soykırımı ve 1990'lı yıllarda PKK ile Türk silahlı kuvvetleri arasındaki çatışma nedeniyle yüzyıllardır yaşadıkları topraklarını terk etmek zorunda kaldı.\", 'HES yapılamayacak', 'Mücadele süreci', 'Ancak Antalya Kültür ve Tabiat Varlıklarını Koruma Kurulu ve HES şirketleri karar itiraz etti.', 'Saat 11:00 Operasyon kapsamında Diyarbakır, İzmir, Van, Ankara, Bitlis gibi illerde de polisler gözaltına alındı.', \"Saat 10:00 Gözaltı sayısı 87'ye çıktı.\", \"İstanbul Terörle Mücadele Şubesi'nin yönetiminde İstanbul merkezli 22 ilde, 200 adrese düzenlenen operasyonda şu ana kadar yaklaşık 52 kişi gözaltına alındı.\", \"Gece 1:50'de başlayan ilk operasyonda 40 kişi gözaltına alındı.\", 'Gözaltına alınanların suç işlemek amacıyla örgüt kurmak, yasa dışı dinleme yapmak ve sahtecilik ile suçlandıkları belirtildi.', 'Gözaltına alınanlar arasında eski Terörle Mücadele Şube Müdürü Yurt Atayün, dönemin Terörle Mücadele Şube Müdürü Ömer Köse de bulunuyor.', 'Dönemin İstanbul İstihbarat Şube Müdürü Ali Fuat Yılmazer ve birçok istihbaratçının evinde arama yapılıyor.', '* Bu haberi gazetelerin internet sitelerinden derledik.', 'Genelkurmay Başkanlığı çatışmada en az altı PYD/PKK mensubunun da öldüğünün değerlendirdiğini söyledi.')]\n",
            "[('governor’s remarks', 'hakur ends his fax saying:', 'timeline of events', '82 taken to prosecution office', 'how did it start?', 'the investigation that came on the agenda july 22 towards the morning hours started in the following way, according to hürriyet newspaper’s news item:', 'inspectors found that the wiretaps had been done with aliases over i̇mei̇ numbers between years 2008-2010, and with real names between 2010-2012.', '* photograph: elif öztürk / i̇stanbul / anadolu agency (aa)', 'demirtaş’s criminal complaint', 'in response to the comment, women shared their laughing photos on twitter.', 'known as a wheat silo for its role in turkey’s grain production, konya closed basin is ecologically one of the 200 most important areas in the world.', '65 percent of konya has dried out', '65 percent of the region’s wetlands have dried out in the past 50 years, according to the world wide fund for nature (wwf) turkey branch and eti̇ burçak cookie company’s research in konya basin.', 'and now it is going through a hydrological drought due to decreasing rainfall throughout the country that further decreases its surface and ground water.', 'illegal wells must be inspected', 'agricultural drought:\\xa0happens when the water demands of plants, pastures, meadows and other agricultural enterprises cannot be met.', 'already 10 billion cubic meters of water deficit', 'dursun yıldız pointed out that water-poor konya basin already needs water to be carried from other places.', 'thermal plant would burden konya', '* 65 percent of wetlands in konya basin have dried out in the past 50 years.', 'ydg-h and ydg-h istanbul also made a declaration saying two had been injured and that the peoples’ democratic party (hdp) was the actual target. (hk/pu)', 'demirtaş: he panicked more than anyone', 'şahin had said, \"the speech in aydın was a misfortune\"', 'i̇brahim şahin had said in his declaration earlier this week:', '* photograph: i̇smihan özgüven / anadolu agency (aa)', 'how many children have died in schools?', 'other questions to be answered in the parliamentary question are as follows:', '* what is the death toll in schools in the past 12 years?', '* in how many lawsuits related to death in schools was your ministry found to be at fault? how much restitution have you had to pay?', 'timeline of events'), ('Valinin açıklamaları', 'Hakur faksını şöyle bitiriyor:', 'Ne olmuştu?', '82 kişi savcılıkta', 'Nasıl başladı?', 'Hürriyet gazetesinin haberine göre, 22 Temmuz sabaha karşı gözaltılarla gündeme gelen soruşturma şöyle başladı:', '2008-2010 tarihleri arasında İMEİ numaraları üzerinden sahte isimlerle dinlemelerin yapıldığı, 2010-2012 tarihleri arasında ise gerçek isimlerle dinlemeler yapıldığı tespit edildi.', '* Fotoğraf: Elif Öztürk / İstanbul / AA', 'Demirtaş suç duyurusunda bulunmuştu', 'Kadınlar, twitter üzerinden kahkaha atan fotoğraflarını paylaştı.', \"Türkiye'nin tahıl üretimindeki yeri nedeniyle buğday ambarı olarak bilinen Konya Kapalı Havzası dünyada ekolojik açıdan en önemli 200 alandan biri.\", \"Konya'nın yüzde 65'i kurudu\", \"Doğal Hayatı Koruma Vakfı WWF-Türkiye ve ETİ Burçak'ın Konya havzasında yaptığı araştırmaya göre 50 yılda bölgedeki sulak alanların yüzde 65'i kurudu.\", 'Şimdi de ülke genelinde azalan yağışlar nedeniyle yer altı ve yer üstü sularını daha da azaltan hidrolojik kuraklık yaşıyor.', 'Kaçak kuyular denetlenmeli', 'Tarımsal kuraklık: Bitkiler ile meralar, çayırlar ve diğer tarımsal işletmelerin su ihtiyaçlarının karşılanamaması.', 'Zaten 10 milyar metreküp su açığı var', 'Dursun Yıldız, su fakiri Konya havzasının zaten ihtiyacı olan suyu başka yerlerden taşımak zorunda olduğuna dikkat çekti.', \"Termik santral Konya'ya yük getirir\", \"*Son 50 yılda Konya Havzası'ndaki sulak alanların yüzde 65'i kurudu.\", 'YDG-H ve YDG-H İstanbul da bir açıklama yaparak iki yaralının olduğunu ve asıl hedefin HDP olduğunu belirtti.(HK)', 'Demirtaş: Herkesten çok paniklemiş', 'Şahin, \"Aydın konuşması talihsizlik\" demişti', 'İbrahim Şahin, hafta başında yaptığı açıklamada şunları söylemişti:', '* Fotoğraf: İsmihan Özgüven / AA', 'Okullarda kaç çocuk öldü?', 'Önergede cevaplanması istenen diğer sorular şöyle:', '* Son 12 yılda okullarda gerçekleşen ölüm sayısı kaçtır?', '* Okullarda yaşanan ölümlerden dolayı açılan davaların kaçında bakanlığınız kusurlu bulunmuştur? Kusurlu bulunduğunuz davalarda ödediğiniz tazminat miktarı ne kadardır?', 'Ne olmuştu?')]\n",
            "[('demirtaş: fair trt', 'i̇bb fired first', 'first, i̇stanbul metropolitan municipality’s subcontractor company fired chamber of city planners board member süleyman balyemez on march 12th, 2012.', 'the reason was city planners presenting their personal objection petitions on the construction plans prepared for taksim square at the press statement taksim solidarity organized in front of i̇bb.', 'during this protest security guards recorded süleyman balyemez’s identity information and one week after the protest they fired him from the job he had for 7 years.', 'beylikdüzü fired during the gezi resistance', 'gürkan akgün said he got fired because of the stand chamber of city planners and taksim solidarity had during the gezi resistance.', 'both members won the reemployment lawsuit and the right to indemnity.', '\"it was a political decision\"', 'tahaoğlu: journalism in not a ‘mannish’, rather a feminized manner', 'the reason of the awarding;', '* due to having a journalist identity that seeks the truth, questions and searches without making a race, gender, language and ethnic discrimination,', 'who is müşerref hekimoğlu?', 'another case for “resisting police officers”', 'the verdict of constitutional court is as below:', 'reason could be a spam attack', 'koç: social media must take precautions against digital lynching attempts', 'spam attacks', 'murat tekin asked demirtaş’s opinion about the statement of avcı.', 'demirtaş responded:', 'erdoğan defended mother tongue when he was abroad.', 'the president erdoğan was on a european tour recently. he said if someone lost his/her mother tongue, s/he would be assimilated.”', '\"we must be very sensitive about this subject because you “think” in your own language.', '\"firstly you will teach your children turkish and make him/her learn local language as well as turkish. our children shouldn’t feel alienated in this land.', 'a woman with a headscarf we talked to said she voted for three different parties throughout her life and she was indecisive whom to vote for.', 'party', 'total', 'rate of votes (%)', 'number of deputies', 'chp'), ('Demirtaş: Adaletli TRT', 'Önce İBB attı', \"Önce 12 Mart 2012'de Şehir Plancılar Odası Yönetim Kurulu Üyesi Süleyman Balyemez, İstanbul Büyükşehir Belediyesi'ne bağlı taşeron firma tarafından işten çıkartıldı.\", \"Gerekçe, Taksim Dayanışması'nın İBB önünde düzenlediği basın açıklamasında Taksim Meydanı için hazırlanan İmar Planlarına yönelik bireysel itiraz dilekçelerini sunmasıydı.\", \"Bu eylem sırasında Süleyman Balyemez 'in kimlik bilgileri güvenlik görevlilerince kayıt altına alınmış ve eylemin ardından bir hafta sonra yedi yıldır çalıştığı işinden çıkartılmıştı.\", 'Gezi sürecinde de Beylikdüzü attı', \"Gürkan Akgün, Taksim Dayanışması bileşeni olan Şehir Plancıları Odası'nın Gezi Parkı sürecindeki duruşu nedeniyle işten çıkarıldığını söylemişti.\", 'İki üye de açtığı davada işe iade davası ve tazminat hakkı kazandı.', '\"Siyasi bir karardı\"', 'Tahaoğlu: Adam gibi değil kadın gibi gazetecilik', \"Çiçek Tahaoğlu'nun ödüllendirilme gerekçesi şöyle:\", '* Toplumda ırk, cins, dil ve etnik köken ayırımı yapmadan gerçeklerin peşinde koşan, sorgulayan ve araştıran gazeteci kimliği,', 'Müşerref Hekimoğlu kimdir?', 'Bir de \"polise direndin\" davası', \"AYM'nin gerekçeli kararı şöyle:\", 'Sebep spam saldırısı olabilir', 'Koç: Sosyal medya siteleri dijital lince önlem almalı', 'Spam saldırıları', 'Bakanın açıklamasıyla ilgili görüşlerini Murat Yetkin sordu.', 'Demirtaş bu soruya şöyle yanıt verdi.', 'Erdoğan yurtdışında anadili savunmuştu', 'Cumhurbaşkanı Erdoğan geçtiğimiz günlerde çıktığı Avrupa turu esnasında yaptığı konuşmada \"Dilimizi kaybedersek diğer değerlerimize sahip çıkamayız\" demişti.', '\"Burada çok hassas olmamız lazım. Anadiliniz unutmayın düşündüğünüz dildir.', '\"Çocuklarınıza en önce Türkçe öğretecek sonra yaşadığımız bölgedeki dili en az onun kadar öğrenmesini sağlayacağız. Artık hiçbir çocuğumuz kendisini bu coğrafyada yabancı olarak görmesin.', 'Kentte konuştuğumuz kişilerden başörtülü bir kadın, hayatı boyunca üç kere farklı partiye oy verdiğini seçimlere çok az bir zaman kalmasına rağmen de hala kime vereceğini bilmediğini söylüyor.', 'Parti', 'Toplam', 'Oy oranı (%)', 'Milletvekili sayısı', 'Cumhuriyet Halk Partisi')]\n",
            "[('241.062', '38,12', '3', 'akp', '224.330', '35,47', '3', 'mhp', '115.087', '18,19', '1', 'independents', '24.078', '3,80', '* this tour has been made with the help of coordinatorship of p24.', 'hdp', 'filiz koçali', 'muzaffer yöndemli', 'sevay akkan açıcı', 'kibar boza', 'mahmut nacar', 'derviş peker', 'ayfer demirel', 'akp', 'mehmet sadık atay', 'abdurrahman öz', 'zeynep armağan uslu', 'mustafa savaş', 'metin yavuz', 'ersin esenlik'), ('241.062', '38,12', '3', 'Adalet ve Kalkınma Partisi', '224.330', '35,47', '3', 'Milliyetçi Hareket Partisi', '115.087', '18,19', '1', 'Bağımsızlar', '24.078', '3,80', '* Bu gezi P24 koordinatörlüğünde gerçekleşti.', 'HDP', 'Filiz Koçali', 'Muzaffer Yöndemli', 'Sevay Akkan Açıcı', 'Kibar Boza', 'Mahmut Nacar', 'Derviş Peker', 'Ayfer Demirel', 'AKP', 'Mehmet Sadık Atay', 'Abdurrahman Öz', 'Zeynep Armağan Uslu', 'Mustafa Savaş', 'Metin Yavuz', 'Ersin Esenlik')]\n",
            "[('cengiz altıntaş', 'chp', 'bülent tezcan', 'metin lütfi baydar', 'hüseyin yıldız', 'mehmet fatih atay', 'tayfun talipoğlu', 'ferda çağlar erkut', 'fulya üstündağ', 'mhp', 'ali uzunırmak', 'hayri güleç', 'hüseyin karagöz', 'deniz depboylu', 'fevzi köse', 'hilmi bolatoğlu', 'musa savaş sent a message from his personal website:', 'what happened?', 'seyhan district co-chair, hüseyin beyaz, former province co-chair, sabahattin pişkinbaş from socialist party of the oppressed (esp) and i̇brahim yakup were injured.', 'explosion in mersin', '* serhan başçuhadar - adana/aa', \"mehmet fırat, hdp's deputy candidate from mersin told bianet he was informed there was an explosion in mersin but he didn't get information about the casualties.\", 'karabulut,\" we left the meeting earlier fortunately. it would have been worse.\"', 'marchers came from far and wide', 'workers demanded rise in wages based on collective labor agreement (ti̇s). they requested none of the workers to be fired due to the strike and resignations to be accepted by trade unions.', 'ti̇s was contracted on dec 14, 2014 between metal workers trade union of turkey (türk metal) and turkey’s metal industrialists union (mess) and the contract includes years between 2014 and 2017.', 'an anonymous worker told bianet:', 'click here to read this article in turkish', 'click here to read this article in turkish', 'who is mehmet akarca?'), ('Cengiz Altıntaş', 'CHP', 'Bülent Tezcan', 'Metin Lütfi Baydar', 'Hüseyin Yıldız', 'Mehmet Fatih Atay', 'Tayfun Talipoğlu', 'Ferda Çağlar Erkut', 'Fulya Üstündağ', 'MHP', 'Ali Uzunırmak', 'Hayri Güleç', 'Hüseyin Karagöz', 'Deniz Depboylu', 'Fevzi Köse', 'Hilmi Bolatoğlu', 'Kendi internet sayfası üzerinden bir mesaj yayımlayan Savaş, şunları söyledi:', 'Ne olmuştu?', \"Seyhan İlçe Başkanı'nın makam odasındaki patlamada ilçe eşbaşkanı Hüseyin Beyaz, ESP eski il başkanı Sabahattin Pişkinbaş ile İbrahim Yakup yaralandı.\", \"Mersin'de patlama\", '* Serhan Başçuhadar - Adana/AA', \"Mersin milletvekili adayı Dengir Mir Mehmet Fırat ise bianet'e yaptığı açıklamada Mersin'de patlama olduğu bilgisini aldığını ama yaralı olmadığı bilgisi geldiğini belirtti.\", 'Seçim toplantılarını erken bitirdiklerini söyleyen Karabulut, \"Aksi takdirde zarar daha da büyük olurdu\" dedi.', 'Yürüyüşe dört bir yandan katılım', 'bianet\\'e konuşan Renault\\'dan bir işçi, \"Artık biz kendimize Harranlı Renocu diyoruz\" diyerek kendi aralarındaki şakalaşmayı anlattı.', \"İşçilerin değişmesini istedikleri TİS, Türk Metal Sendikası ile işveren sendikası Türkiye Metal Sanayicileri Sendikası (MESS) arasında Aralık 2014'te imzalanmıştı ve 2014-2017 yıllarını kapsıyor.\", 'Üç talepten ikisi karşılandı', 'TOFAŞ üretimi durdurduğunu açıkladı', 'Olcayto şöyle devam etti:', 'Mehmet Akarca kimdir?')]\n"
          ]
        }
      ],
      "source": [
        "for batch_num, batch in enumerate(iterator):\n",
        "    print(batch)\n",
        "    if batch_num > 3:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1071,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the padding token\n",
        "PADDING_TOKEN = ''\n",
        "START_TOKEN = ''\n",
        "END_TOKEN = ''\n",
        "\n",
        "\n",
        "# Ensure the padding token is included in the Turkish vocabulary\n",
        "if PADDING_TOKEN not in turkish_vocabulary:\n",
        "    turkish_vocabulary.append(PADDING_TOKEN)\n",
        "\n",
        "# Create mappings for Turkish vocabulary\n",
        "index_to_turkish = {k: v for k, v in enumerate(turkish_vocabulary)}\n",
        "turkish_to_index = {v: k for k, v in enumerate(turkish_vocabulary)}\n",
        "\n",
        "# Create mappings for English vocabulary\n",
        "index_to_english = {k: v for k, v in enumerate(english_vocabulary)}\n",
        "english_to_index = {v: k for k, v in enumerate(english_vocabulary)}\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "# Define the criterion with the correct ignore_index\n",
        "criterian = nn.CrossEntropyLoss(ignore_index=turkish_to_index[PADDING_TOKEN],\n",
        "                                reduction='none')\n",
        "\n",
        "# When computing the loss, we are ignoring cases when the label is the padding token\n",
        "for params in transformer.parameters():\n",
        "    if params.dim() > 1:\n",
        "        nn.init.xavier_uniform_(params)\n",
        "\n",
        "optim = torch.optim.Adam(transformer.parameters(), lr=1e-4)\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1072,
      "metadata": {},
      "outputs": [],
      "source": [
        "NEG_INFTY = -1e9\n",
        "\n",
        "def create_masks(eng_batch, tr_batch):\n",
        "    num_sentences = len(eng_batch)\n",
        "    look_ahead_mask = torch.full([max_sequence_length, max_sequence_length] , True)\n",
        "    look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)\n",
        "    encoder_padding_mask = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
        "    decoder_padding_mask_self_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
        "    decoder_padding_mask_cross_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
        "\n",
        "    for idx in range(num_sentences):\n",
        "      eng_sentence_length, tr_sentence_length = len(eng_batch[idx]), len(tr_batch[idx])\n",
        "      eng_chars_to_padding_mask = np.arange(eng_sentence_length + 1, max_sequence_length)\n",
        "      tr_chars_to_padding_mask = np.arange(tr_sentence_length + 1, max_sequence_length)\n",
        "      encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = True\n",
        "      encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = True\n",
        "      decoder_padding_mask_self_attention[idx, :, tr_chars_to_padding_mask] = True\n",
        "      decoder_padding_mask_self_attention[idx, tr_chars_to_padding_mask, :] = True\n",
        "      decoder_padding_mask_cross_attention[idx, :, eng_chars_to_padding_mask] = True\n",
        "      decoder_padding_mask_cross_attention[idx, tr_chars_to_padding_mask, :] = True\n",
        "\n",
        "    encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0)\n",
        "    decoder_self_attention_mask =  torch.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n",
        "    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n",
        "    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1073,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add the missing character to the vocabulary\n",
        "missing_char = '’'\n",
        "if missing_char not in turkish_vocabulary:\n",
        "    turkish_vocabulary.append(missing_char)\n",
        "    index_to_turkish[len(turkish_vocabulary) - 1] = missing_char\n",
        "    turkish_to_index[missing_char] = len(turkish_vocabulary) - 1\n",
        "\n",
        "if missing_char not in english_vocabulary:\n",
        "    english_vocabulary.append(missing_char)\n",
        "    index_to_english[len(english_vocabulary) - 1] = missing_char\n",
        "    english_to_index[missing_char] = len(english_vocabulary) - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdgtTSKvwN9_"
      },
      "source": [
        "Modify mask such that the padding tokens cannot look ahead.\n",
        "In Encoder, tokens before it should be -1e9 while tokens after it should be -inf.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLcXI4wkMLck"
      },
      "source": [
        "Note the target mask starts with 2 rows of non masked items: https://github.com/SamLynnEvans/Transformer/blob/master/Beam.py#L55\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1074,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab 'ü': 252\n",
            "Dataset 'ü': 252\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "vocab_char = 'ü'\n",
        "dataset_char = 'ü' # datasetdan kelayotgan belgini yozing yoki olib ko‘ring batch[1][sentence_num][x]\n",
        "\n",
        "print(\"Vocab 'ü':\", ord(vocab_char))\n",
        "print(\"Dataset 'ü':\", ord(dataset_char))\n",
        "print(vocab_char == dataset_char)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1075,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[90, 93, 88, 77, 85, 1, 98, 80, 95, 82]\n"
          ]
        }
      ],
      "source": [
        "sentence = \"örnek üğşı\"\n",
        "tokenized = [turkish_to_index[char] for char in sentence]\n",
        "print(tokenized)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1076,
      "metadata": {
        "vscode": {
          "languageId": "ruby"
        }
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "keyword argument repeated: tr_vocab_size (1107686551.py, line 3)",
          "output_type": "error",
          "traceback": [
            "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1076]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mtr_vocab_size=len(turkish_vocabulary),\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m keyword argument repeated: tr_vocab_size\n"
          ]
        }
      ],
      "source": [
        "transformer = Transformer(\n",
        "    tr_vocab_size=len(english_vocabulary),\n",
        "    tr_vocab_size=len(turkish_vocabulary),\n",
        "    src_pad_idx=english_to_index[PADDING_TOKEN],\n",
        "    trg_pad_idx=turkish_to_index[PADDING_TOKEN],\n",
        "    english_to_index=english_to_index,\n",
        "    turkish_to_index=turkish_to_index,\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'ü'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[940]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_batch, kn_batch)\n\u001b[32m     13\u001b[39m optim.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m tr_predictions = \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43meng_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mtr_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mencoder_self_attention_mask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mdecoder_self_attention_mask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mdecoder_cross_attention_mask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m                             \u001b[49m\u001b[43menc_start_token\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m                             \u001b[49m\u001b[43menc_end_token\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mdec_start_token\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mdec_end_token\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m labels = transformer.decoder.sentence_embedding.batch_tokenize(tr_batch, start_token=\u001b[38;5;28;01mFalse\u001b[39;00m, end_token=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     24\u001b[39m loss = criterian(\n\u001b[32m     25\u001b[39m     tr_predictions.view(-\u001b[32m1\u001b[39m, tr_vocab_size).to(device),\n\u001b[32m     26\u001b[39m     labels.view(-\u001b[32m1\u001b[39m).to(device)\n\u001b[32m     27\u001b[39m ).to(device)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Maftuna\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Maftuna\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Maftuna\\Downloads\\transformer.py:301\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, x, y, encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask, enc_start_token, enc_end_token, dec_start_token, dec_end_token)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[32m    292\u001b[39m             x, \n\u001b[32m    293\u001b[39m             y, \n\u001b[32m   (...)\u001b[39m\u001b[32m    299\u001b[39m             dec_start_token=\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;66;03m# We should make this true\u001b[39;00m\n\u001b[32m    300\u001b[39m             dec_end_token=\u001b[38;5;28;01mFalse\u001b[39;00m): \u001b[38;5;66;03m# x, y are batch of sentences\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m301\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_self_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43menc_start_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43menc_end_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    302\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.decoder(x, y, decoder_self_attention_mask, decoder_cross_attention_mask, start_token=dec_start_token, end_token=dec_end_token)\n\u001b[32m    303\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.linear(out)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Maftuna\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Maftuna\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Maftuna\\Downloads\\transformer.py:178\u001b[39m, in \u001b[36mEncoder.forward\u001b[39m\u001b[34m(self, x, self_attention_mask, start_token, end_token)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, self_attention_mask, start_token, end_token):\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msentence_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.layers(x, self_attention_mask)\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Maftuna\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Maftuna\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Maftuna\\Downloads\\transformer.py:70\u001b[39m, in \u001b[36mSentenceEmbedding.forward\u001b[39m\u001b[34m(self, x, start_token, end_token)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, start_token, end_token): \u001b[38;5;66;03m# sentence\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.embedding(x)\n\u001b[32m     72\u001b[39m     pos = \u001b[38;5;28mself\u001b[39m.position_encoder().to(get_device())\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Maftuna\\Downloads\\transformer.py:65\u001b[39m, in \u001b[36mSentenceEmbedding.batch_tokenize\u001b[39m\u001b[34m(self, batch, start_token, end_token)\u001b[39m\n\u001b[32m     63\u001b[39m tokenized = []\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sentence_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(batch)):\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m    tokenized.append( \u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43msentence_num\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_token\u001b[49m\u001b[43m)\u001b[49m )\n\u001b[32m     66\u001b[39m tokenized = torch.stack(tokenized)\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized.to(get_device())\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Maftuna\\Downloads\\transformer.py:54\u001b[39m, in \u001b[36mSentenceEmbedding.batch_tokenize.<locals>.tokenize\u001b[39m\u001b[34m(sentence, start_token, end_token)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtokenize\u001b[39m(sentence, start_token, end_token):\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     sentence_word_indicies = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlanguage_to_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(sentence)]\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m start_token:\n\u001b[32m     56\u001b[39m         sentence_word_indicies.insert(\u001b[32m0\u001b[39m, \u001b[38;5;28mself\u001b[39m.language_to_index[\u001b[38;5;28mself\u001b[39m.START_TOKEN])\n",
            "\u001b[31mKeyError\u001b[39m: 'ü'"
          ]
        }
      ],
      "source": [
        "transformer.train()\n",
        "transformer.to(device)\n",
        "total_loss = 0\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch}\")\n",
        "    iterator = iter(train_loader)\n",
        "    for batch_num, batch in enumerate(iterator):\n",
        "        transformer.train()\n",
        "        eng_batch, tr_batch = batch\n",
        "        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_batch, kn_batch)\n",
        "        optim.zero_grad()\n",
        "        tr_predictions = transformer(eng_batch,\n",
        "                                     tr_batch,\n",
        "                                     encoder_self_attention_mask.to(device), \n",
        "                                     decoder_self_attention_mask.to(device), \n",
        "                                     decoder_cross_attention_mask.to(device),\n",
        "                                     enc_start_token=False,\n",
        "                                     enc_end_token=False,\n",
        "                                     dec_start_token=True,\n",
        "                                     dec_end_token=True)\n",
        "        labels = transformer.decoder.sentence_embedding.batch_tokenize(tr_batch, start_token=False, end_token=True)\n",
        "        loss = criterian(\n",
        "            tr_predictions.view(-1, tr_vocab_size).to(device),\n",
        "            labels.view(-1).to(device)\n",
        "        ).to(device)\n",
        "        valid_indicies = torch.where(labels.view(-1) == turkish_to_index[PADDING_TOKEN], False, True)\n",
        "        loss = loss.sum() / valid_indicies.sum()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        #train_losses.append(loss.item())\n",
        "        if batch_num % 100 == 0:\n",
        "            print(f\"Iteration {batch_num} : {loss.item()}\")\n",
        "            print(f\"English: {eng_batch[0]}\")\n",
        "            print(f\"Turkish Translation: {tr_batch[0]}\")\n",
        "            tr_sentence_predicted = torch.argmax(tr_predictions[0], axis=1)\n",
        "            predicted_sentence = \"\"\n",
        "            for idx in tr_sentence_predicted:\n",
        "              if idx == turkish_to_index[END_TOKEN]:\n",
        "                break\n",
        "              predicted_sentence += index_to_turkish[idx.item()]\n",
        "            print(f\"Turkish Prediction: {predicted_sentence}\")\n",
        "\n",
        "\n",
        "            transformer.eval()\n",
        "            tr_sentence = (\"\",)\n",
        "            eng_sentence = (\"should we go to the mall?\",)\n",
        "            for word_counter in range(max_sequence_length):\n",
        "                encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask= create_masks(eng_sentence, kn_sentence)\n",
        "                predictions = transformer(eng_sentence,\n",
        "                                          tr_sentence,\n",
        "                                          encoder_self_attention_mask.to(device), \n",
        "                                          decoder_self_attention_mask.to(device), \n",
        "                                          decoder_cross_attention_mask.to(device),\n",
        "                                          enc_start_token=False,\n",
        "                                          enc_end_token=False,\n",
        "                                          dec_start_token=True,\n",
        "                                          dec_end_token=False)\n",
        "                next_token_prob_distribution = predictions[0][word_counter] # not actual probs\n",
        "                next_token_index = torch.argmax(next_token_prob_distribution).item()\n",
        "                next_token = index_to_turkish[next_token_index]\n",
        "                tr_sentence = (tr_sentence[0] + next_token, )\n",
        "                if next_token == END_TOKEN:\n",
        "                  break\n",
        "            \n",
        "            print(f\"Evaluation translation (should we go to the mall?) : {tr_sentence}\")\n",
        "            print(\"-------------------------------------------\")\n",
        "     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nosVPGVijId"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOQe-juylBiJ"
      },
      "outputs": [],
      "source": [
        "transformer.eval()\n",
        "def translate(eng_sentence):\n",
        "  eng_sentence = (eng_sentence,)\n",
        "  kn_sentence = (\"\",)\n",
        "  for word_counter in range(max_sequence_length):\n",
        "    encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask= create_masks(eng_sentence, kn_sentence)\n",
        "    predictions = transformer(eng_sentence,\n",
        "                              kn_sentence,\n",
        "                              encoder_self_attention_mask.to(device), \n",
        "                              decoder_self_attention_mask.to(device), \n",
        "                              decoder_cross_attention_mask.to(device),\n",
        "                              enc_start_token=False,\n",
        "                              enc_end_token=False,\n",
        "                              dec_start_token=True,\n",
        "                              dec_end_token=False)\n",
        "    next_token_prob_distribution = predictions[0][word_counter]\n",
        "    next_token_index = torch.argmax(next_token_prob_distribution).item()\n",
        "    next_token = index_to_turkish[next_token_index]\n",
        "    kn_sentence = (kn_sentence[0] + next_token, )\n",
        "    if next_token == END_TOKEN:\n",
        "      break\n",
        "  return kn_sentence[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDVH_YsxlK6q",
        "outputId": "83c47f99-53c0-4c2d-c26a-aaa426f50563"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "00000çççç00000ç:::0000::::000000000000000000000000000000000000000000000000000000ç30000000000000000000000000000000:::::::00000000000000000000çç:::::0000000:::::0000::::::::::00000000000000000000000ç3ee\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"what should we do when the day starts?\")\n",
        "print(translation)\n",
        "#ದಿನ ಪ್ರಾರಂಭವಾದಾಗ ನಾವು ಏನು ಮಾಡಬೇಕು?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9yfawBnul0W",
        "outputId": "d9e6e6b7-683b-45f9-f013-c53c31038306"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ಹೇಗೆ ಇದು ಹೇಗೆ ಹೇಗೆ?<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"how is this the truth?\")\n",
        "print(translation)\n",
        "#ಇದು ಹೇಗೆ ಸತ್ಯ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpdYBk5-urcQ",
        "outputId": "ca7249c5-efda-4f41-f052-ecef9691be82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ಇದರಿಂದ ಮೂಲಕ ಸಂಬಂಧಿಸಿದ ಮೇಲೆ ಮಾಡಿದ್ದಾರೆ<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"the world is a large place with different people\")\n",
        "print(translation)\n",
        "#ಪ್ರಪಂಚವು ವಿಭಿನ್ನ ಜನರೊಂದಿಗೆ ದೊಡ್ಡ ಸ್ಥಳವಾಗಿದೆ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ni9e2UYUuxi3",
        "outputId": "b93968e6-3f12-4794-b277-3a3821af221e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ನಾನು ಕುಟುಂಬದ ಹೆಸರು<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"my name is ajay\")\n",
        "print(translation)\n",
        "#ನನ್ನ ಹೆಸರು ಅಜಯ್"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJuJKHqFldW3",
        "outputId": "71aa2c6c-ec77-4b02-d39b-bd724012fb53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ನಾನು ಅಂತರ ಸಂಗತಿ ನಾನು ಕೊಡುವುದಿಲ್ಲ<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"i cannot stand this smell\")\n",
        "print(translation)\n",
        "#ನಾನು ಈ ವಾಸನೆಯನ್ನು ಸಹಿಸುವುದಿಲ್ಲ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxHC4Lirlfu8",
        "outputId": "5a3ca401-abac-41af-d9db-99fb7a57fe00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ಯಾವುದೇ ಕಾರಣಗಳು ಇಲ್ಲ<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"noodles are the best\")\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLVOSI0Oli16",
        "outputId": "9f9445a1-2802-4688-900c-d7ecf2bd5c35"
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "index out of range in self",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[777]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m translation = \u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwhy care about this?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(translation)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[283]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mtranslate\u001b[39m\u001b[34m(eng_sentence)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m word_counter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_sequence_length):\n\u001b[32m      6\u001b[39m   encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask= create_masks(eng_sentence, kn_sentence)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m   predictions = \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43meng_sentence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mkn_sentence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mencoder_self_attention_mask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mdecoder_self_attention_mask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mdecoder_cross_attention_mask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m                            \u001b[49m\u001b[43menc_start_token\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m                            \u001b[49m\u001b[43menc_end_token\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mdec_start_token\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mdec_end_token\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m   next_token_prob_distribution = predictions[\u001b[32m0\u001b[39m][word_counter]\n\u001b[32m     17\u001b[39m   next_token_index = torch.argmax(next_token_prob_distribution).item()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Maftuna\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Maftuna\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Maftuna\\Downloads\\transformer.py:301\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, x, y, encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask, enc_start_token, enc_end_token, dec_start_token, dec_end_token)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[32m    292\u001b[39m             x, \n\u001b[32m    293\u001b[39m             y, \n\u001b[32m   (...)\u001b[39m\u001b[32m    299\u001b[39m             dec_start_token=\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;66;03m# We should make this true\u001b[39;00m\n\u001b[32m    300\u001b[39m             dec_end_token=\u001b[38;5;28;01mFalse\u001b[39;00m): \u001b[38;5;66;03m# x, y are batch of sentences\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m301\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_self_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43menc_start_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43menc_end_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    302\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.decoder(x, y, decoder_self_attention_mask, decoder_cross_attention_mask, start_token=dec_start_token, end_token=dec_end_token)\n\u001b[32m    303\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.linear(out)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Maftuna\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Maftuna\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Maftuna\\Downloads\\transformer.py:178\u001b[39m, in \u001b[36mEncoder.forward\u001b[39m\u001b[34m(self, x, self_attention_mask, start_token, end_token)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, self_attention_mask, start_token, end_token):\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msentence_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.layers(x, self_attention_mask)\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Maftuna\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Maftuna\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Maftuna\\Downloads\\transformer.py:71\u001b[39m, in \u001b[36mSentenceEmbedding.forward\u001b[39m\u001b[34m(self, x, start_token, end_token)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, start_token, end_token): \u001b[38;5;66;03m# sentence\u001b[39;00m\n\u001b[32m     70\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.batch_tokenize(x, start_token, end_token)\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m     pos = \u001b[38;5;28mself\u001b[39m.position_encoder().to(get_device())\n\u001b[32m     73\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.dropout(x + pos)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Maftuna\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Maftuna\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Maftuna\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Maftuna\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\functional.py:2551\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2545\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2546\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2547\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2548\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2549\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2550\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mIndexError\u001b[39m: index out of range in self"
          ]
        }
      ],
      "source": [
        "translation = translate(\"why care about this?\")\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MStWCoAt0Ixp"
      },
      "source": [
        "This translated pretty well : \"What is the reason. Why\" without punctuation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AB6TEJfGlkRT",
        "outputId": "adb465d5-b0ed-4be4-9251-62c079f49491"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ಇದು ಅತ್ಯಂತ ಹೊರತಾಗಿದೆ<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"this is the best thing ever\")\n",
        "print(translation)\n",
        "# ಇದು ಎಂದೆಂದಿಗೂ ಉತ್ತಮವಾಗಿದೆ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxsUjSybxYkh"
      },
      "source": [
        "The translation : \"This is very unusual\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQwDbuWBlmmA",
        "outputId": "7ae0e2c0-02c0-4c74-bc47-26b67da55a06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ನಾನು ಕೇಳಿದ್ದೇನೆ.<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"i am here\")\n",
        "print(translation)\n",
        "# ನಾನು ಇಲ್ಲಿದ್ದೇನೆ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmyZ2-I6x0Yf"
      },
      "source": [
        "Translation: \"I have heard\". \n",
        "This is why word based translator may perform better than character translator. This is actually very good at optimizing the objective of the current transformer even though the translation is off."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ifeV4bGluIj",
        "outputId": "6bce922d-d0db-432c-e6c6-97d482f1dea6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ಕ್ಯಾನ್ ಕ್ಲಿಕ್ ಕ್ಲಿಕ್ ಕ್ಲಿಕ್ ಕ್ಲಿಕ್ ಕ್ಲಿಕ್ ಮಾಡಿ<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"click this\")\n",
        "print(translation)\n",
        "# ಇದನ್ನು ಕ್ಲಿಕ್ ಮಾಡಿ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RB5DUBEl1kD",
        "outputId": "9109e504-8b9e-45dc-e13c-e878b3741e4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ಎಲ್ಲಿ ಎಲ್ಲಿ ಎಲ್ಲಿ?<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"where is the mall?\")\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdeJ9CvMn5LM",
        "outputId": "044b5dac-29a9-4b60-e66a-4e10739a9756"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ಏನು ಮಾಡಬೇಕು?<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"what should we do?\")\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoikFnov1rj-"
      },
      "source": [
        "This is correct; but it absolutely fumbles on the next one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GFeyzrg1fIZ",
        "outputId": "2b4dfb04-def2-4725-9e76-5476bf85e2ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ಅದನ್ನು ಮಾಡಿದ ಮೇಲೆ ಮಾಡಿದ ಮಾಡಿದ್ದಾರೆ<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"today, what should we do\")\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0upygLS-sXcO",
        "outputId": "53a62435-ab16-4d4c-8a9f-0bf07af2a501"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ಅದರ ಮೇಲೆ ಏಕೆ ಮಾಡಿದ್ದಾರೆ?<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"why did they activate?\")\n",
        "print(translation)\n",
        "# ಅವರು ಏಕೆ ಸಕ್ರಿಯಗೊಳಿಸಿದರು?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSrsqEGmtcl2",
        "outputId": "1b8b8faa-5370-426a-f2f2-fe852696bf7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ಅವರು ಏಕೆ ಅವರು ಏಕೆ ಮಾಡಿದ್ದಾರೆ?<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"why did they do this?\")\n",
        "print(translation)\n",
        "# ಅವರು ಇದನ್ನು ಏಕೆ ಮಾಡಿದರು?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7ISM5rd3BLJ"
      },
      "source": [
        "That turned out well!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjTcH2HFtyld",
        "outputId": "9dea5498-f139-4cbd-ee8c-6108e1b92fc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ನಾನು ನಾನು ಕೊಡುತ್ತೇನೆ.<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"i am well.\")\n",
        "print(translation)\n",
        "# ನಾನು ಆರಾಮವಾಗಿದ್ದೇನೆ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP0YX2g74eP7"
      },
      "source": [
        "Translation: \"I will give you something\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pGdN13kt5Br",
        "outputId": "240256c5-f594-41b0-8218-2c70a22a156f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ಇದರ ಬಗ್ಗೆ ಏನು?<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"whats the word on the street?\")\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFYZ6pOe4o-X"
      },
      "source": [
        "Kind of close semantically. Translation is something like: \"What is this about\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzrOcNUk1-e5"
      },
      "source": [
        "## Insights\n",
        "\n",
        "- When training, we can treat every alphabet as a single unit instead of splitting it into it's corresponding parts to preserve meaning. For example, ಮಾ should be 1 unit when comuting a loss. It should not be decomposed into ಮ + ఆ\n",
        "- Using word-based or BPE based tokenizations may help mitigate (1). Also, we will get valid word (or BPE) units if we do so. \n",
        "- Make sure the training set has a large variety of sentences that are not just about one topic like \"work\" and \"government\"\n",
        "- Increase the number of encoder / decoder units for better translations. It was set to the minimum of 1 of each unit here.\n",
        "- Create a translator with a language you understand ideally."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd6_k0Uu5V7f"
      },
      "source": [
        "Overall, this model definately learned something. And you can use other languages instead of this kannada language and might see better luck"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
